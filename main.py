# -*- coding: utf-8 -*-
"""CVPR_Final_project(_6106_6097).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i9Gu31krLImCFgIp4hzQlZfM2oa2KC5n

# 🌿 **CVPR Project:  Plant Disease Detection System**

---

- **Atharva Bhalerao**
**https://www.linkedin.com/in/atharva-bhalerao-b62787298/**






##📘 Introduction

In the vast ecosystem of global agriculture 🌾, plant health plays a pivotal role in ensuring food security and economic stability. However, plants are constantly under threat from a wide variety of diseases—ranging from fungal infections and bacterial blights to viral invasions 🦠. These diseases, often invisible to the naked eye in their early stages, can devastate crops, reduce yields, and inflict billions of dollars in losses annually 🌍💸.

Conventional methods of disease detection rely heavily on human expertise 👨‍🌾 and manual inspection, which can be time-consuming, error-prone, and inaccessible in remote or under-resourced regions. This is where Artificial Intelligence (AI) steps in—as a digital botanist that never sleeps 🧠🤖.

By harnessing the power of Deep Learning, particularly Convolutional Neural Networks (CNNs), I aim to mimic the human ability to visually detect disease symptoms, but with greater speed, consistency, and scalability. Using the widely recognized Plant Village dataset, which contains thousands of labeled leaf images, this project seeks to revolutionize the way we approach plant disease diagnosis.

##🎯 Objective

The core objective of this project is to design and develop a highly accurate, automated plant disease detection system using deep learning technologies 🌟.

## aim to:

🧠 Train a Convolutional Neural Network (CNN) to recognize and classify diseases based on visual symptoms in leaf images.

🖼️ Utilize the Plant Village dataset, which includes a diverse range of healthy and infected plant leaves, to ensure the model learns from real-world variations.

⚙️ Create a model that can generalize well across different lighting conditions, leaf orientations, and backgrounds—mimicking how a skilled agronomist would diagnose a plant in the field.

🌱 Empower farmers, agronomists, and agricultural workers with a reliable, mobile-ready solution to detect diseases early and take informed action.

By the end of this project, we envision a world where smartphones become plant doctors, capable of identifying diseases with just a snapshot 📸—saving crops, time, and livelihoods in the process.

# 🛠 **Libraries and Frameworks Used**

## 1. TensorFlow and Keras 📊
 - TensorFlow: Open-source framework for machine learning and deep learning.
 - Keras: Simplifies building, training, and evaluating CNN models (integrated with TensorFlow).

## 2. Pandas and NumPy 📈
 - Pandas: For data manipulation and organization.
 - NumPy: For numerical operations, arrays, and matrices.

## 3. Matplotlib and Seaborn 📊
 - Matplotlib: Visualizes data and tracks model training.
 - Seaborn: High-level statistical visualizations (e.g., heatmaps).

## 4. Scikit-learn 🧠
- Scikit-learn: Used for dataset splitting, performance metrics, and confusion matrix generation.

## 5. Image Processing 🖼
 - PIL: For image data manipulation (opening, modifying, saving).
 - ImageDataGenerator: Performs real-time data augmentation to prevent overfitting.
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adamax
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import BatchNormalization
from tqdm import tqdm
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

"""#**🌱 Dataset**
🔗 Dataset Link: New Plant Diseases Dataset
Understanding any real-world problem begins with the right data—and in our case, that means leaves 🍃. The dataset used in this project is a rich and diverse collection of plant leaf images, carefully curated to represent real agricultural conditions.

📂 What’s Inside?
This dataset consists of thousands of high-resolution images, categorized by:

🌿 Plant species (e.g., Tomato, Apple, Corn, Grape, etc.)

🦠 Disease types (e.g., Leaf Spot, Rust, Mildew, Mosaic Virus)

✅ Healthy vs Diseased leaves

Each class includes multiple samples that showcase various stages and manifestations of disease—like yellowing, wilting, mold, and discoloration—ensuring the model learns to identify even subtle symptoms with accuracy 🎯.

🎯 Why this dataset?
The inclusion of both healthy and infected leaves allows us to build a balanced and robust model capable of not just identifying disease, but distinguishing between similar symptoms across plant types. This is critical for real-world deployment, where visual cues might be ambiguous or overlapping 👀.

🧠 Whether used for model training, validation, or testing, this dataset provides the foundation on which our AI learns to become a smart plant pathologist 🤖🌿
"""

train_dir = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'
valid_dir = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'
test_dir = '/kaggle/input/new-plant-diseases-dataset/test/test'

!pip install opendatasets

import opendatasets as od
import pandas
od.download(
    "https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset?select=test")

# {"username":"atharvabhalerao9423","key":"atharvabhalerao9423"}

"""# Collecting file paths and their corresponding labels from training directory"""

filenames_train = []
label_train = []
folds = os.listdir(train_dir)
for fold in folds:
    FoldPath = os.path.join(train_dir, fold)
    files = os.listdir(FoldPath)
    for file in tqdm(files):
        filepath = os.path.join(FoldPath,file)
        filenames_train.append(filepath)
        label_train.append(fold)

"""<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Collecting file paths and their corresponding labels from valid directory

</div>
"""

filenames_valid = []
label_valid = []
folds = os.listdir(valid_dir)
for fold in folds:
    FoldPath = os.path.join(valid_dir, fold)
    files = os.listdir(FoldPath)
    for file in tqdm(files):
        filepath = os.path.join(FoldPath,file)
        filenames_valid.append(filepath)
        label_valid.append(fold)

"""<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Creating DataFrames for training and validation data

</div>

"""

df_train = pd.DataFrame({
    'filename': filenames_train,
    'label': label_train
})
df_valid = pd.DataFrame({
    'filename': filenames_valid,
    'label': label_valid
})

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Explore data
</div>"""

print(df_train.shape)
print(df_valid.shape)

df_train.head(5)

color = ['#CAE0BC','#780C28','#EAFAEA','#6E8E59']

plt.figure(figsize=(30,9))
plt.bar(df_train['label'].unique(), df_train['label'].value_counts(), color=color)
plt.title('Train Data Distribution')
plt.xticks(rotation=45)
plt.show()

print(np.unique(label_train))

"""## **Data Preprocessing and Augmentation**

---
To improve model performance and generalization, we apply data preprocessing techniques such as image normalization and augmentation. This ensures that the model learns robust features from diverse variations of plant leaf images.

**Image Normalization and Augmentation**

**Rescaling:** Normalizes pixel values to a range of [0,1] by dividing by 255.

**Rotation:** Randomly rotates images up to 20 degrees.

**Shifting:**Allows slight horizontal and vertical shifts to introduce variations.

**Flipping:** Randomly flips images horizontally to increase diversity.

**Validation Split:** Splits the dataset into 80% training and 20% validation.
"""

data_gen  = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

# only normalization
test_gen = ImageDataGenerator(rescale=1./255)


imge_size = (224,224)
batch_size = 32

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Creating data generators from DataFrames for training and validation
</div>
"""

train_gen = data_gen.flow_from_dataframe(
    df_train,
    x_col='filename',
    shuffle=True,
    y_col='label',
    target_size=(imge_size[0],imge_size[1]),
    class_mode='categorical',
    batch_size=batch_size)

valid_gen = data_gen.flow_from_dataframe(
    df_valid,
    shuffle=True,
    x_col='filename',
    y_col='label',
    target_size=(imge_size[0],imge_size[1]),
    class_mode='categorical')

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Retrieving class indices mapping from the training generator
</div>"""

class_dict = train_gen.class_indices
print(class_dict)

"""#1.Convolutional Neural Network (CNN) 🧠💡

Convolutional Neural Network (CNN) Overview for Plant Disease Detection 🌿

A CNN is a deep learning model designed for image recognition tasks, ideal for detecting plant diseases from images by identifying hierarchical patterns.

##Key Features:

Convolutional Layers 🌀: Apply filters to detect essential patterns like edges and textures, helping identify plant features such as spots or discoloration.

Pooling Layers 🔽: Reduce image size while retaining important features, making the model faster and reducing overfitting.

Fully Connected Layers 🔗: Connect detected features to produce the final classification (e.g., healthy vs. diseased).

Activation Functions ⚡: ReLU introduces non-linearity, enabling the model to learn complex patterns, such as those in irregular plant diseases.

##Strengths:

Excellent for image-related tasks like plant disease detection 🌾.

Flexible and customizable architecture 🛠️.

Great at learning hierarchical patterns in data.

##Weaknesses:

Computationally expensive for large datasets 💻.

Requires significant fine-tuning for complex tasks 🧑‍🔬.

Usage in Plant Disease Detection: CNNs excel in recognizing plant disease patterns but may need extensive tuning and computational resources for complex datasets.









"""

Model = Sequential([
    Conv2D(64, kernel_size= (3,3),padding='same', activation='relu', input_shape=(imge_size[0],imge_size[1],3)),
    Conv2D(64, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Conv2D(128, kernel_size= (3,3),padding='same', activation='relu'),
    Conv2D(128, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),


    Conv2D(128, kernel_size= (3,3),padding='same', activation='relu'),
    Conv2D(128, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Conv2D(256, kernel_size= (3,3),padding='same', activation='relu'),
    Conv2D(256, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Conv2D(512, kernel_size= (3,3),padding='same', activation='relu'),
    Conv2D(512, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Conv2D(512, kernel_size= (3,3),padding='same', activation='relu'),
    Conv2D(512, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Flatten(),
    Dense(256, activation='relu'),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(38, activation='softmax') ])

"""#Displaying CNN Model Architecture Summary 🧠📊
The CNN model summary provides an overview of the layers, output shapes, and parameters, helping to understand the model’s structure and complexity.

Purpose 🎯: It allows you to inspect:

Layer Types (e.g., Conv2D, MaxPooling2D, Dense)

Number of Parameters at each layer (indicating model complexity)

Output Shapes (ensuring correct dimensional transformations)

What It Shows:

Layer Name 🏗️: Type of layer (e.g., Conv2D, Dense).

Output Shape 🔄: Data dimensions after the layer.

Param # ⚙️: Number of parameters (weights and biases).

Total Parameters 💡: Total number of trainable and non-trainable parameters.

Why Use It?

Model Debugging 🛠️: Ensures correct architecture, especially with dimensionality changes.

Model Complexity 📏: Helps assess complexity, computational cost, and potential for overfitting.
"""

Model.summary()

Model.compile(Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Training the CNN model with early stopping
</div>
"""

history = Model.fit(
    train_gen,
    epochs=40,
    validation_data=valid_gen,
    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],
    verbose=1

    )

test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    image_size=(imge_size[0], imge_size[1]),
    shuffle=False,
    labels=None
)

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
      Evaluating the CNN model on training and validation data
</div>"""

print(Model.evaluate(train_gen))
print(Model.evaluate(valid_gen))

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Saving the trained CNN model ACC 97.23%
</div>
"""

Model.save('Modelplanit_ACC97.23.h5')

"""<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
    Making predictions on the validation dataset and extracting class labels
</div>

Here’s the theory for the code snippet you provided:

---

### **Model Prediction and Argmax for Class Labels 🧠📊**

In this part of the code, the model makes predictions on a validation dataset, and the predicted probabilities are then converted into class labels. Here’s what’s happening:

- **Model Prediction on Validation Set 🔍**:
  - `preds = Model.predict(valid_gen)`: This line runs the model's prediction on the entire validation dataset (`valid_gen`). The `Model.predict()` function returns a list of probabilities for each class for every image in the validation set.

- **Converting Probabilities to Class Labels 🎯**:
  - `y_pred = np.argmax(preds, axis=1)`: Once the model has predicted probabilities for each class, `np.argmax()` is used to get the index of the class with the highest probability for each image. This index corresponds to the predicted class label. By using `axis=1`, we ensure that we’re applying `argmax` across the class dimension (not the individual samples).

### **Why Use Argmax?**
The model returns probabilities for each class, but the class with the highest probability is the predicted label. Using `np.argmax()`, we extract the index of that class, effectively assigning a label to each image in the validation set.

This process helps transform the model’s output (probabilities) into the actual predicted class labels, which can be used for evaluation and comparison against the true labels.
"""

preds = Model.predict(valid_gen)
y_pred = np.argmax(preds, axis=1)

"""#  Evaluating Model Performance Using a Confusion Matrix 📊

---

After training the model, it’s essential to evaluate how well it performs on the test dataset. One of the most effective ways to analyze classification performance is by using a **confusion matrix**.

### **What is a Confusion Matrix?**
A confusion matrix is a table used to describe the performance of a classification model. It shows:

- **True Positives (TP) ✅** – Correctly predicted disease class.
- **False Positives (FP) ❌** – Incorrectly predicted a disease when it was something else.
- **False Negatives (FN) ⚠️** – Missed a disease (misclassified as another class).
- **True Negatives (TN) ✅** – Correctly identified as NOT belonging to a class.

### **Why use it?**
It helps visualize model performance for multi-class classification problems.
It also shows how often the model confuses one disease with another. 💡

"""

cm = confusion_matrix(valid_gen.classes, y_pred)
labels = list(class_dict.keys())
plt.figure(figsize=(19,12))
sns.heatmap(cm, annot=True, fmt='d', cmap=color, xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted Label')
plt.ylabel('Truth Label')
plt.show()

"""
---

### **Plotting Model Accuracy Over Epochs 📈**

To monitor a machine learning model’s performance, plot the training accuracy and validation accuracy over epochs:

Training Accuracy 🚀: Shows how well the model performs on training data as it learns.

Validation Accuracy 📊: Measures the model’s performance on unseen data to ensure it's generalizing well.

Key insights:

Overfitting: High training accuracy, but validation accuracy plateaus or drops.

Underfitting: Both accuracies are low or stagnant."""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

"""<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     predict the class of an input image and visualize probabilities
</div>

---

### **Image Prediction and Visualization 📸🔍**

The `predict` function is used to make predictions on an input image, and it provides both the visual representation of the image and a probability distribution over possible class labels. Here’s what each part of the function does:

- **Image Preprocessing 🖼️**:
  The image is loaded and resized to match the input size required by the model (224x224 pixels). The image is then converted to a numpy array and normalized by dividing by 255, ensuring pixel values are scaled between 0 and 1, which is typically needed for deep learning models.

- **Model Prediction 🧠**:
  The model makes a prediction on the preprocessed image. This is done using the `Model.predict()` function, which outputs a list of probabilities for each possible class. The higher the probability, the more confident the model is that the image belongs to that class.

- **Visualization 📊**:
  The image is displayed using `matplotlib` to give a visual context for the prediction. Below the image, a horizontal bar chart is plotted to represent the probability distribution across all possible classes. Each bar shows the likelihood that the image belongs to a particular class, with the exact probability labeled on the bars.

- **Result Interpretation 💡**:
  The predicted class can be inferred by looking at the class with the highest probability. The chart gives a detailed view of the model's uncertainty, showing how it ranks different classes.

This function helps in both visualizing the input image and understanding the confidence levels of the model for each potential class label.
"""

def predict(img_path):


    label = list(class_dict.keys())
    plt.figure(figsize=(12, 19))
    img = Image.open(img_path)
    resized_img = img.resize((224, 224))
    img = np.asarray(resized_img)
    img = np.expand_dims(img, axis=0)
    img = img / 255
    predictions = Model.predict(img)
    probs = list(predictions[0])
    labels = label
    plt.subplot(2, 1, 1)
    plt.imshow(resized_img)
    plt.subplot(2, 1, 2)
    bars = plt.barh(labels, probs)
    plt.xlabel('Probability', fontsize=15)
    ax = plt.gca()
    ax.bar_label(bars, fmt = '%.2f')
    plt.show()

predict('/kaggle/input/new-plant-diseases-dataset/test/test/AppleCedarRust2.JPG')

"""#**EfficientNet B3 ⚡**🔍

EfficientNet B3 is a cutting-edge deep learning model that achieves high accuracy with optimized resource usage. It's part of the EfficientNet family known for its smart scaling strategy and powerful performance.

🔑 Key Features:
Compound Scaling 🔄: Scales depth, width, and resolution together for better accuracy and efficiency.

Pretrained on ImageNet 🧑‍🏫: Enables strong transfer learning with minimal data.

Lightweight & Fast ⚡: Optimized for both training speed and deployment.

Efficient Parameters 🔧: Achieves better performance with fewer parameters.

✅ Strengths:
High Accuracy 📈 – Achieved 99.52% on our dataset.

Faster Convergence ⏳ – Trains quickly compared to traditional CNNs.

Better Generalization 🧩 – Performs well on unseen, varied plant images.

⚠️ Weakness:
Slightly higher computational needs 🖥️ than basic CNNs, but still efficient for its complexity.

🌿 In Our Project:
EfficientNet B3 proved ideal for detecting a wide range of plant diseases, offering top-tier accuracy and robust performance across varied conditions.



"""

base_model_b3 = tf.keras.applications.EfficientNetB3(
    include_top=False,
    weights="imagenet",
    input_shape=(imge_size[0], imge_size[1], 3),
    pooling='max',
)

"""#⚡🌿 Building & Compiling EfficientNetB3 Model
EfficientNetB3, a pre-trained model on ImageNet, is used as the feature extractor for plant disease detection. It offers an excellent balance of accuracy and computational efficiency ⚖️.

#🚀 Why EfficientNetB3?
Combines depth, width, and resolution scaling.

Delivers high accuracy with fewer parameters.

Ideal for transfer learning on image classification tasks.

##🧠 How It Works:
Transfer Learning: Base EfficientNetB3 is frozen; only top layers are trained.

Global Average Pooling 🌍: Reduces feature map size while preserving key info.

Fully Connected Layers 🔗: Learn final classification mappings.

Dropout 🎲: Reduces overfitting and improves generalization.

Softmax Layer 🔮: Outputs probability distribution over disease classes.

This architecture ensures fast, accurate, and resource-efficient training—perfect for real-time plant disease detection. ✅


"""

ModelPretrained = Sequential([
    base_model,

    Dense(256 , activation='relu'),
    Dropout(0.3),
    Dense(38, activation='softmax')

])
ModelPretrained.compile(optimizer=Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

"""<div style="background-color: black; color: gold; padding: 10px;
            font-size: 20px; border-radius: 5px; text-align: center;">
     Training the fine-tuned EfficientNetB3 model with training and validation data
</div>

"""

history_Final = ModelPretrained.fit(
    train_gen,
    epochs=5,
    validation_data=valid_gen,
    verbose=1,

)

"""#**#Evaluating the Fine-Tuned EfficientNetB3 Model 📊🔍**


After training EfficientNetB3, we evaluated its performance on both training and validation data to ensure strong generalization and avoid overfitting.

📈 Key Metrics:
Accuracy 🎯: Measures correct predictions across all classes.

Loss 💥: Categorical cross-entropy indicates how close predictions are to true labels.

Confusion Matrix 📊: Visualizes how well the model distinguishes between disease classes.

Precision, Recall, F1-Score 💡: Provide detailed insights, especially for imbalanced datasets.


"""

print(ModelPretrained.evaluate(train_gen))
print(ModelPretrained.evaluate(valid_gen))

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Saving the trained CNN model ACC 99.53%
</div>
"""

ModelPretrained.save('Plant_Village_Detection_Model_ACC99.53%.h5')

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Generating predictions using the fine-tuned EfficientNetB3 model on validation data
</div>
"""

preds = ModelPretrained.predict(valid_gen)
y_pred_ModelPretrained = np.argmax(preds, axis=1)

"""#  **Evaluating Model Performance Using a Confusion Matrix 📊**

---

After training the model, it’s essential to evaluate how well it performs on the test dataset. One of the most effective ways to analyze classification performance is by using a **confusion matrix**.

### **What is a Confusion Matrix?**
A confusion matrix is a table used to describe the performance of a classification model. It shows:

- **True Positives (TP) ✅** – Correctly predicted disease class.
- **False Positives (FP) ❌** – Incorrectly predicted a disease when it was something else.
- **False Negatives (FN) ⚠️** – Missed a disease (misclassified as another class).
- **True Negatives (TN) ✅** – Correctly identified as NOT belonging to a class.

### **Why use it?**
It helps visualize model performance for multi-class classification problems.
It also shows how often the model confuses one disease with another. 💡

"""

cm = confusion_matrix(valid_gen.classes, y_pred_ModelPretrained)
labels = list(class_dict.keys())
plt.figure(figsize=(19,12))
sns.heatmap(cm, annot=True, fmt='d', cmap=color, xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted Label')
plt.ylabel('Truth Label')
plt.show()

"""<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     predict the class of an input image and visualize probabilities
</div>
"""

def predict(img_path):


    label = list(class_dict.keys())
    plt.figure(figsize=(12, 19))
    img = Image.open(img_path)
    resized_img = img.resize((224, 224))
    img = np.asarray(resized_img)
    img = np.expand_dims(img, axis=0)
    img = img / 255
    predictions = ModelPretrained.predict(img)
    probs = list(predictions[0])
    labels = label
    plt.subplot(2, 1, 1)
    plt.imshow(resized_img)
    plt.subplot(2, 1, 2)
    bars = plt.barh(labels, probs)
    plt.xlabel('Probability', fontsize=15)
    ax = plt.gca()
    ax.bar_label(bars, fmt = '%.2f')
    plt.show()

predict('/kaggle/input/new-plant-diseases-dataset/test/test/TomatoYellowCurlVirus2.JPG')

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Plotting the training and validation accuracy and loss over epochs

</div>"""

plt.plot(history_Final.history['accuracy'])
plt.plot(history_Final.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

"""<div style="background-color: black; color: gold; padding: 15px;
            font-size: 18px; border-radius: 8px;  font-weight: bold;">
    🌿 The Plant Village model is now complete!<br>  
    🎯 Powered by EfficientNetB3, it accurately classifies plant diseases, aiding in early detection.  <br>
    🚀 Ready to revolutionize smart agriculture and support farmers worldwide!  <br>

</div>

#**Model Comparison: CNN vs. EfficientNetB3 🌿📊**

# **Model Comparison for Plant Disease Detection 🌱🔬**

| **Criteria**                | **CNN (Accuracy: 0.9765) 🧠**                                    | **EfficientNet B3 (Accuracy: 0.9952) ⚡**                      |
|-----------------------------|------------------------------------------------------------------|---------------------------------------------------------------|
| **Architecture**             | Custom architecture designed for specific tasks.                | Pretrained architecture optimized for efficient performance.  |
| **Accuracy 📊**              | 97.65% – High accuracy, but may have limitations on more complex data. | 99.52% – Slightly higher accuracy, showing better performance on plant disease detection. |
| **Training Time ⏳**         | Generally requires more epochs to converge.                     | Faster convergence due to advanced architecture.              |
| **Parameter Count ⚙️**      | Fewer parameters, which may reduce computational cost.           | Higher parameter count but more optimized for efficiency.     |
| **Computational Efficiency 💻** | Less efficient on large datasets, can be slower.                | Highly efficient, especially on large datasets, thanks to the design of EfficientNet. |
| **Overfitting 🔴**           | Can be prone to overfitting with small datasets.                 | Less prone to overfitting, even with a higher number of parameters. |
| **Generalization 🧩**        | May have lower generalization capabilities compared to EfficientNet B3. | Excellent generalization, benefiting from pretraining and its design. |
| **Suitability for Small Datasets 🌾** | Adaptable for smaller datasets with proper tuning.              | Suitable for both small and large datasets due to transfer learning. |

#Which is Best? 🤔

**EfficientNet B3 ⚡**:
With an accuracy of 99.52%, it clearly outperforms the CNN model. It is likely the better choice for plant disease detection due to its superior accuracy, efficient use of parameters, and strong generalization abilities.

**CNN🧠: ** While achieving 97.65% accuracy, it performs slightly lower than EfficientNet B3. It could still be a solid option for simpler or smaller datasets, but for optimal results in detecting plant diseases, EfficientNet B3 would likely offer the best performance.

Conclusion:
With EfficientNet B3 providing a higher accuracy (99.52%), it is the recommended model for your project, especially if the goal is to maximize performance on complex datasets like plant disease detection.

---

### **Short Model Comparison (✔️ vs. ❌)**

| **Metric**                        | **CNN Model**              | **EfficientNetB3 Model**       |
|-----------------------------------|----------------------------|--------------------------------|
| **Accuracy 🏆**                    | ❌ Lower (0.9765)           | ✔️ Higher (0.9952)            |
| **Training Time ⏳**                | ❌ Longer                  | ✔️ Faster (due to transfer learning) |
| **Generalization 🌍**              | ❌ May overfit              | ✔️ Excellent generalization  |
| **Computational Efficiency ⚡**    | ❌ Higher resource use      | ✔️ More efficient             |
| **Ease of Implementation 🧑‍💻**    | ❌ Requires more customization | ✔️ Easy (pre-trained model)   |
| **Overfitting Risk ⚠️**           | ❌ Higher                   | ✔️ Lower                      |
| **Resource Consumption 💻**        | ❌ High                     | ✔️ Low                        |

---

### **Best Choice**: **EfficientNetB3** ✔️

#**✅ Conclusion**
In this project, we successfully developed a plant disease detection system using deep learning techniques. Two models were implemented and evaluated: a custom CNN and the EfficientNet B3 architecture.

The CNN model achieved an impressive accuracy of 97.65%, demonstrating its capability in identifying diseases from leaf images.

The EfficientNet B3 model outperformed the CNN with a remarkable accuracy of 99.52%, proving its efficiency, scalability, and superior generalization.

EfficientNet B3’s use of compound scaling, Swish activation, and squeeze-and-excitation blocks contributed to its high performance with optimized resource usage.

This project shows that deep learning models, especially EfficientNet B3, are powerful tools for real-time plant disease diagnosis. Such systems can be integrated
into mobile or web applications to assist farmers and agricultural experts, ultimately helping to increase crop yield, reduce losses,
and promote smarter farming practices.
"""
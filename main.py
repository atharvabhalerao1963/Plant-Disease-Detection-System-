# -*- coding: utf-8 -*-
"""CVPR_Final_project(_6106_6097).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i9Gu31krLImCFgIp4hzQlZfM2oa2KC5n

# ğŸŒ¿ **CVPR Project:  Plant Disease Detection System**

---

- **Atharva Bhalerao**
**https://www.linkedin.com/in/atharva-bhalerao-b62787298/**






##ğŸ“˜ Introduction

In the vast ecosystem of global agriculture ğŸŒ¾, plant health plays a pivotal role in ensuring food security and economic stability. However, plants are constantly under threat from a wide variety of diseasesâ€”ranging from fungal infections and bacterial blights to viral invasions ğŸ¦ . These diseases, often invisible to the naked eye in their early stages, can devastate crops, reduce yields, and inflict billions of dollars in losses annually ğŸŒğŸ’¸.

Conventional methods of disease detection rely heavily on human expertise ğŸ‘¨â€ğŸŒ¾ and manual inspection, which can be time-consuming, error-prone, and inaccessible in remote or under-resourced regions. This is where Artificial Intelligence (AI) steps inâ€”as a digital botanist that never sleeps ğŸ§ ğŸ¤–.

By harnessing the power of Deep Learning, particularly Convolutional Neural Networks (CNNs), I aim to mimic the human ability to visually detect disease symptoms, but with greater speed, consistency, and scalability. Using the widely recognized Plant Village dataset, which contains thousands of labeled leaf images, this project seeks to revolutionize the way we approach plant disease diagnosis.

##ğŸ¯ Objective

The core objective of this project is to design and develop a highly accurate, automated plant disease detection system using deep learning technologies ğŸŒŸ.

## aim to:

ğŸ§  Train a Convolutional Neural Network (CNN) to recognize and classify diseases based on visual symptoms in leaf images.

ğŸ–¼ï¸ Utilize the Plant Village dataset, which includes a diverse range of healthy and infected plant leaves, to ensure the model learns from real-world variations.

âš™ï¸ Create a model that can generalize well across different lighting conditions, leaf orientations, and backgroundsâ€”mimicking how a skilled agronomist would diagnose a plant in the field.

ğŸŒ± Empower farmers, agronomists, and agricultural workers with a reliable, mobile-ready solution to detect diseases early and take informed action.

By the end of this project, we envision a world where smartphones become plant doctors, capable of identifying diseases with just a snapshot ğŸ“¸â€”saving crops, time, and livelihoods in the process.

# ğŸ›  **Libraries and Frameworks Used**

## 1. TensorFlow and Keras ğŸ“Š
 - TensorFlow: Open-source framework for machine learning and deep learning.
 - Keras: Simplifies building, training, and evaluating CNN models (integrated with TensorFlow).

## 2. Pandas and NumPy ğŸ“ˆ
 - Pandas: For data manipulation and organization.
 - NumPy: For numerical operations, arrays, and matrices.

## 3. Matplotlib and Seaborn ğŸ“Š
 - Matplotlib: Visualizes data and tracks model training.
 - Seaborn: High-level statistical visualizations (e.g., heatmaps).

## 4. Scikit-learn ğŸ§ 
- Scikit-learn: Used for dataset splitting, performance metrics, and confusion matrix generation.

## 5. Image Processing ğŸ–¼
 - PIL: For image data manipulation (opening, modifying, saving).
 - ImageDataGenerator: Performs real-time data augmentation to prevent overfitting.
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adamax
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import BatchNormalization
from tqdm import tqdm
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

"""#**ğŸŒ± Dataset**
ğŸ”— Dataset Link: New Plant Diseases Dataset
Understanding any real-world problem begins with the right dataâ€”and in our case, that means leaves ğŸƒ. The dataset used in this project is a rich and diverse collection of plant leaf images, carefully curated to represent real agricultural conditions.

ğŸ“‚ Whatâ€™s Inside?
This dataset consists of thousands of high-resolution images, categorized by:

ğŸŒ¿ Plant species (e.g., Tomato, Apple, Corn, Grape, etc.)

ğŸ¦  Disease types (e.g., Leaf Spot, Rust, Mildew, Mosaic Virus)

âœ… Healthy vs Diseased leaves

Each class includes multiple samples that showcase various stages and manifestations of diseaseâ€”like yellowing, wilting, mold, and discolorationâ€”ensuring the model learns to identify even subtle symptoms with accuracy ğŸ¯.

ğŸ¯ Why this dataset?
The inclusion of both healthy and infected leaves allows us to build a balanced and robust model capable of not just identifying disease, but distinguishing between similar symptoms across plant types. This is critical for real-world deployment, where visual cues might be ambiguous or overlapping ğŸ‘€.

ğŸ§  Whether used for model training, validation, or testing, this dataset provides the foundation on which our AI learns to become a smart plant pathologist ğŸ¤–ğŸŒ¿
"""

train_dir = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'
valid_dir = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'
test_dir = '/kaggle/input/new-plant-diseases-dataset/test/test'

!pip install opendatasets

import opendatasets as od
import pandas
od.download(
    "https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset?select=test")

# {"username":"atharvabhalerao9423","key":"atharvabhalerao9423"}

"""# Collecting file paths and their corresponding labels from training directory"""

filenames_train = []
label_train = []
folds = os.listdir(train_dir)
for fold in folds:
    FoldPath = os.path.join(train_dir, fold)
    files = os.listdir(FoldPath)
    for file in tqdm(files):
        filepath = os.path.join(FoldPath,file)
        filenames_train.append(filepath)
        label_train.append(fold)

"""<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Collecting file paths and their corresponding labels from valid directory

</div>
"""

filenames_valid = []
label_valid = []
folds = os.listdir(valid_dir)
for fold in folds:
    FoldPath = os.path.join(valid_dir, fold)
    files = os.listdir(FoldPath)
    for file in tqdm(files):
        filepath = os.path.join(FoldPath,file)
        filenames_valid.append(filepath)
        label_valid.append(fold)

"""<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Creating DataFrames for training and validation data

</div>

"""

df_train = pd.DataFrame({
    'filename': filenames_train,
    'label': label_train
})
df_valid = pd.DataFrame({
    'filename': filenames_valid,
    'label': label_valid
})

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Explore data
</div>"""

print(df_train.shape)
print(df_valid.shape)

df_train.head(5)

color = ['#CAE0BC','#780C28','#EAFAEA','#6E8E59']

plt.figure(figsize=(30,9))
plt.bar(df_train['label'].unique(), df_train['label'].value_counts(), color=color)
plt.title('Train Data Distribution')
plt.xticks(rotation=45)
plt.show()

print(np.unique(label_train))

"""## **Data Preprocessing and Augmentation**

---
To improve model performance and generalization, we apply data preprocessing techniques such as image normalization and augmentation. This ensures that the model learns robust features from diverse variations of plant leaf images.

**Image Normalization and Augmentation**

**Rescaling:** Normalizes pixel values to a range of [0,1] by dividing by 255.

**Rotation:** Randomly rotates images up to 20 degrees.

**Shifting:**Allows slight horizontal and vertical shifts to introduce variations.

**Flipping:** Randomly flips images horizontally to increase diversity.

**Validation Split:** Splits the dataset into 80% training and 20% validation.
"""

data_gen  = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

# only normalization
test_gen = ImageDataGenerator(rescale=1./255)


imge_size = (224,224)
batch_size = 32

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Creating data generators from DataFrames for training and validation
</div>
"""

train_gen = data_gen.flow_from_dataframe(
    df_train,
    x_col='filename',
    shuffle=True,
    y_col='label',
    target_size=(imge_size[0],imge_size[1]),
    class_mode='categorical',
    batch_size=batch_size)

valid_gen = data_gen.flow_from_dataframe(
    df_valid,
    shuffle=True,
    x_col='filename',
    y_col='label',
    target_size=(imge_size[0],imge_size[1]),
    class_mode='categorical')

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Retrieving class indices mapping from the training generator
</div>"""

class_dict = train_gen.class_indices
print(class_dict)

"""#1.Convolutional Neural Network (CNN) ğŸ§ ğŸ’¡

Convolutional Neural Network (CNN) Overview for Plant Disease Detection ğŸŒ¿

A CNN is a deep learning model designed for image recognition tasks, ideal for detecting plant diseases from images by identifying hierarchical patterns.

##Key Features:

Convolutional Layers ğŸŒ€: Apply filters to detect essential patterns like edges and textures, helping identify plant features such as spots or discoloration.

Pooling Layers ğŸ”½: Reduce image size while retaining important features, making the model faster and reducing overfitting.

Fully Connected Layers ğŸ”—: Connect detected features to produce the final classification (e.g., healthy vs. diseased).

Activation Functions âš¡: ReLU introduces non-linearity, enabling the model to learn complex patterns, such as those in irregular plant diseases.

##Strengths:

Excellent for image-related tasks like plant disease detection ğŸŒ¾.

Flexible and customizable architecture ğŸ› ï¸.

Great at learning hierarchical patterns in data.

##Weaknesses:

Computationally expensive for large datasets ğŸ’».

Requires significant fine-tuning for complex tasks ğŸ§‘â€ğŸ”¬.

Usage in Plant Disease Detection: CNNs excel in recognizing plant disease patterns but may need extensive tuning and computational resources for complex datasets.









"""

Model = Sequential([
    Conv2D(64, kernel_size= (3,3),padding='same', activation='relu', input_shape=(imge_size[0],imge_size[1],3)),
    Conv2D(64, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Conv2D(128, kernel_size= (3,3),padding='same', activation='relu'),
    Conv2D(128, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),


    Conv2D(128, kernel_size= (3,3),padding='same', activation='relu'),
    Conv2D(128, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Conv2D(256, kernel_size= (3,3),padding='same', activation='relu'),
    Conv2D(256, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Conv2D(512, kernel_size= (3,3),padding='same', activation='relu'),
    Conv2D(512, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Conv2D(512, kernel_size= (3,3),padding='same', activation='relu'),
    Conv2D(512, kernel_size= (3,3),padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2,2)),

    Flatten(),
    Dense(256, activation='relu'),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(38, activation='softmax') ])

"""#Displaying CNN Model Architecture Summary ğŸ§ ğŸ“Š
The CNN model summary provides an overview of the layers, output shapes, and parameters, helping to understand the modelâ€™s structure and complexity.

Purpose ğŸ¯: It allows you to inspect:

Layer Types (e.g., Conv2D, MaxPooling2D, Dense)

Number of Parameters at each layer (indicating model complexity)

Output Shapes (ensuring correct dimensional transformations)

What It Shows:

Layer Name ğŸ—ï¸: Type of layer (e.g., Conv2D, Dense).

Output Shape ğŸ”„: Data dimensions after the layer.

Param # âš™ï¸: Number of parameters (weights and biases).

Total Parameters ğŸ’¡: Total number of trainable and non-trainable parameters.

Why Use It?

Model Debugging ğŸ› ï¸: Ensures correct architecture, especially with dimensionality changes.

Model Complexity ğŸ“: Helps assess complexity, computational cost, and potential for overfitting.
"""

Model.summary()

Model.compile(Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Training the CNN model with early stopping
</div>
"""

history = Model.fit(
    train_gen,
    epochs=40,
    validation_data=valid_gen,
    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],
    verbose=1

    )

test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    image_size=(imge_size[0], imge_size[1]),
    shuffle=False,
    labels=None
)

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
      Evaluating the CNN model on training and validation data
</div>"""

print(Model.evaluate(train_gen))
print(Model.evaluate(valid_gen))

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Saving the trained CNN model ACC 97.23%
</div>
"""

Model.save('Modelplanit_ACC97.23.h5')

"""<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
    Making predictions on the validation dataset and extracting class labels
</div>

Hereâ€™s the theory for the code snippet you provided:

---

### **Model Prediction and Argmax for Class Labels ğŸ§ ğŸ“Š**

In this part of the code, the model makes predictions on a validation dataset, and the predicted probabilities are then converted into class labels. Hereâ€™s whatâ€™s happening:

- **Model Prediction on Validation Set ğŸ”**:
  - `preds = Model.predict(valid_gen)`: This line runs the model's prediction on the entire validation dataset (`valid_gen`). The `Model.predict()` function returns a list of probabilities for each class for every image in the validation set.

- **Converting Probabilities to Class Labels ğŸ¯**:
  - `y_pred = np.argmax(preds, axis=1)`: Once the model has predicted probabilities for each class, `np.argmax()` is used to get the index of the class with the highest probability for each image. This index corresponds to the predicted class label. By using `axis=1`, we ensure that weâ€™re applying `argmax` across the class dimension (not the individual samples).

### **Why Use Argmax?**
The model returns probabilities for each class, but the class with the highest probability is the predicted label. Using `np.argmax()`, we extract the index of that class, effectively assigning a label to each image in the validation set.

This process helps transform the modelâ€™s output (probabilities) into the actual predicted class labels, which can be used for evaluation and comparison against the true labels.
"""

preds = Model.predict(valid_gen)
y_pred = np.argmax(preds, axis=1)

"""#  Evaluating Model Performance Using a Confusion Matrix ğŸ“Š

---

After training the model, itâ€™s essential to evaluate how well it performs on the test dataset. One of the most effective ways to analyze classification performance is by using a **confusion matrix**.

### **What is a Confusion Matrix?**
A confusion matrix is a table used to describe the performance of a classification model. It shows:

- **True Positives (TP) âœ…** â€“ Correctly predicted disease class.
- **False Positives (FP) âŒ** â€“ Incorrectly predicted a disease when it was something else.
- **False Negatives (FN) âš ï¸** â€“ Missed a disease (misclassified as another class).
- **True Negatives (TN) âœ…** â€“ Correctly identified as NOT belonging to a class.

### **Why use it?**
It helps visualize model performance for multi-class classification problems.
It also shows how often the model confuses one disease with another. ğŸ’¡

"""

cm = confusion_matrix(valid_gen.classes, y_pred)
labels = list(class_dict.keys())
plt.figure(figsize=(19,12))
sns.heatmap(cm, annot=True, fmt='d', cmap=color, xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted Label')
plt.ylabel('Truth Label')
plt.show()

"""
---

### **Plotting Model Accuracy Over Epochs ğŸ“ˆ**

To monitor a machine learning modelâ€™s performance, plot the training accuracy and validation accuracy over epochs:

Training Accuracy ğŸš€: Shows how well the model performs on training data as it learns.

Validation Accuracy ğŸ“Š: Measures the modelâ€™s performance on unseen data to ensure it's generalizing well.

Key insights:

Overfitting: High training accuracy, but validation accuracy plateaus or drops.

Underfitting: Both accuracies are low or stagnant."""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

"""<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     predict the class of an input image and visualize probabilities
</div>

---

### **Image Prediction and Visualization ğŸ“¸ğŸ”**

The `predict` function is used to make predictions on an input image, and it provides both the visual representation of the image and a probability distribution over possible class labels. Hereâ€™s what each part of the function does:

- **Image Preprocessing ğŸ–¼ï¸**:
  The image is loaded and resized to match the input size required by the model (224x224 pixels). The image is then converted to a numpy array and normalized by dividing by 255, ensuring pixel values are scaled between 0 and 1, which is typically needed for deep learning models.

- **Model Prediction ğŸ§ **:
  The model makes a prediction on the preprocessed image. This is done using the `Model.predict()` function, which outputs a list of probabilities for each possible class. The higher the probability, the more confident the model is that the image belongs to that class.

- **Visualization ğŸ“Š**:
  The image is displayed using `matplotlib` to give a visual context for the prediction. Below the image, a horizontal bar chart is plotted to represent the probability distribution across all possible classes. Each bar shows the likelihood that the image belongs to a particular class, with the exact probability labeled on the bars.

- **Result Interpretation ğŸ’¡**:
  The predicted class can be inferred by looking at the class with the highest probability. The chart gives a detailed view of the model's uncertainty, showing how it ranks different classes.

This function helps in both visualizing the input image and understanding the confidence levels of the model for each potential class label.
"""

def predict(img_path):


    label = list(class_dict.keys())
    plt.figure(figsize=(12, 19))
    img = Image.open(img_path)
    resized_img = img.resize((224, 224))
    img = np.asarray(resized_img)
    img = np.expand_dims(img, axis=0)
    img = img / 255
    predictions = Model.predict(img)
    probs = list(predictions[0])
    labels = label
    plt.subplot(2, 1, 1)
    plt.imshow(resized_img)
    plt.subplot(2, 1, 2)
    bars = plt.barh(labels, probs)
    plt.xlabel('Probability', fontsize=15)
    ax = plt.gca()
    ax.bar_label(bars, fmt = '%.2f')
    plt.show()

predict('/kaggle/input/new-plant-diseases-dataset/test/test/AppleCedarRust2.JPG')

"""#**EfficientNet B3 âš¡**ğŸ”

EfficientNet B3 is a cutting-edge deep learning model that achieves high accuracy with optimized resource usage. It's part of the EfficientNet family known for its smart scaling strategy and powerful performance.

ğŸ”‘ Key Features:
Compound Scaling ğŸ”„: Scales depth, width, and resolution together for better accuracy and efficiency.

Pretrained on ImageNet ğŸ§‘â€ğŸ«: Enables strong transfer learning with minimal data.

Lightweight & Fast âš¡: Optimized for both training speed and deployment.

Efficient Parameters ğŸ”§: Achieves better performance with fewer parameters.

âœ… Strengths:
High Accuracy ğŸ“ˆ â€“ Achieved 99.52% on our dataset.

Faster Convergence â³ â€“ Trains quickly compared to traditional CNNs.

Better Generalization ğŸ§© â€“ Performs well on unseen, varied plant images.

âš ï¸ Weakness:
Slightly higher computational needs ğŸ–¥ï¸ than basic CNNs, but still efficient for its complexity.

ğŸŒ¿ In Our Project:
EfficientNet B3 proved ideal for detecting a wide range of plant diseases, offering top-tier accuracy and robust performance across varied conditions.



"""

base_model_b3 = tf.keras.applications.EfficientNetB3(
    include_top=False,
    weights="imagenet",
    input_shape=(imge_size[0], imge_size[1], 3),
    pooling='max',
)

"""#âš¡ğŸŒ¿ Building & Compiling EfficientNetB3 Model
EfficientNetB3, a pre-trained model on ImageNet, is used as the feature extractor for plant disease detection. It offers an excellent balance of accuracy and computational efficiency âš–ï¸.

#ğŸš€ Why EfficientNetB3?
Combines depth, width, and resolution scaling.

Delivers high accuracy with fewer parameters.

Ideal for transfer learning on image classification tasks.

##ğŸ§  How It Works:
Transfer Learning: Base EfficientNetB3 is frozen; only top layers are trained.

Global Average Pooling ğŸŒ: Reduces feature map size while preserving key info.

Fully Connected Layers ğŸ”—: Learn final classification mappings.

Dropout ğŸ²: Reduces overfitting and improves generalization.

Softmax Layer ğŸ”®: Outputs probability distribution over disease classes.

This architecture ensures fast, accurate, and resource-efficient trainingâ€”perfect for real-time plant disease detection. âœ…


"""

ModelPretrained = Sequential([
    base_model,

    Dense(256 , activation='relu'),
    Dropout(0.3),
    Dense(38, activation='softmax')

])
ModelPretrained.compile(optimizer=Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

"""<div style="background-color: black; color: gold; padding: 10px;
            font-size: 20px; border-radius: 5px; text-align: center;">
     Training the fine-tuned EfficientNetB3 model with training and validation data
</div>

"""

history_Final = ModelPretrained.fit(
    train_gen,
    epochs=5,
    validation_data=valid_gen,
    verbose=1,

)

"""#**#Evaluating the Fine-Tuned EfficientNetB3 Model ğŸ“ŠğŸ”**


After training EfficientNetB3, we evaluated its performance on both training and validation data to ensure strong generalization and avoid overfitting.

ğŸ“ˆ Key Metrics:
Accuracy ğŸ¯: Measures correct predictions across all classes.

Loss ğŸ’¥: Categorical cross-entropy indicates how close predictions are to true labels.

Confusion Matrix ğŸ“Š: Visualizes how well the model distinguishes between disease classes.

Precision, Recall, F1-Score ğŸ’¡: Provide detailed insights, especially for imbalanced datasets.


"""

print(ModelPretrained.evaluate(train_gen))
print(ModelPretrained.evaluate(valid_gen))

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Saving the trained CNN model ACC 99.53%
</div>
"""

ModelPretrained.save('Plant_Village_Detection_Model_ACC99.53%.h5')

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Generating predictions using the fine-tuned EfficientNetB3 model on validation data
</div>
"""

preds = ModelPretrained.predict(valid_gen)
y_pred_ModelPretrained = np.argmax(preds, axis=1)

"""#  **Evaluating Model Performance Using a Confusion Matrix ğŸ“Š**

---

After training the model, itâ€™s essential to evaluate how well it performs on the test dataset. One of the most effective ways to analyze classification performance is by using a **confusion matrix**.

### **What is a Confusion Matrix?**
A confusion matrix is a table used to describe the performance of a classification model. It shows:

- **True Positives (TP) âœ…** â€“ Correctly predicted disease class.
- **False Positives (FP) âŒ** â€“ Incorrectly predicted a disease when it was something else.
- **False Negatives (FN) âš ï¸** â€“ Missed a disease (misclassified as another class).
- **True Negatives (TN) âœ…** â€“ Correctly identified as NOT belonging to a class.

### **Why use it?**
It helps visualize model performance for multi-class classification problems.
It also shows how often the model confuses one disease with another. ğŸ’¡

"""

cm = confusion_matrix(valid_gen.classes, y_pred_ModelPretrained)
labels = list(class_dict.keys())
plt.figure(figsize=(19,12))
sns.heatmap(cm, annot=True, fmt='d', cmap=color, xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted Label')
plt.ylabel('Truth Label')
plt.show()

"""<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     predict the class of an input image and visualize probabilities
</div>
"""

def predict(img_path):


    label = list(class_dict.keys())
    plt.figure(figsize=(12, 19))
    img = Image.open(img_path)
    resized_img = img.resize((224, 224))
    img = np.asarray(resized_img)
    img = np.expand_dims(img, axis=0)
    img = img / 255
    predictions = ModelPretrained.predict(img)
    probs = list(predictions[0])
    labels = label
    plt.subplot(2, 1, 1)
    plt.imshow(resized_img)
    plt.subplot(2, 1, 2)
    bars = plt.barh(labels, probs)
    plt.xlabel('Probability', fontsize=15)
    ax = plt.gca()
    ax.bar_label(bars, fmt = '%.2f')
    plt.show()

predict('/kaggle/input/new-plant-diseases-dataset/test/test/TomatoYellowCurlVirus2.JPG')

"""
<div style="background-color: black; color: gold; padding: 10px; text-align: center; font-size: 20px; border-radius: 5px;">
     Plotting the training and validation accuracy and loss over epochs

</div>"""

plt.plot(history_Final.history['accuracy'])
plt.plot(history_Final.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')

"""<div style="background-color: black; color: gold; padding: 15px;
            font-size: 18px; border-radius: 8px;  font-weight: bold;">
    ğŸŒ¿ The Plant Village model is now complete!<br>  
    ğŸ¯ Powered by EfficientNetB3, it accurately classifies plant diseases, aiding in early detection.  <br>
    ğŸš€ Ready to revolutionize smart agriculture and support farmers worldwide!  <br>

</div>

#**Model Comparison: CNN vs. EfficientNetB3 ğŸŒ¿ğŸ“Š**

# **Model Comparison for Plant Disease Detection ğŸŒ±ğŸ”¬**

| **Criteria**                | **CNN (Accuracy: 0.9765) ğŸ§ **                                    | **EfficientNet B3 (Accuracy: 0.9952) âš¡**                      |
|-----------------------------|------------------------------------------------------------------|---------------------------------------------------------------|
| **Architecture**             | Custom architecture designed for specific tasks.                | Pretrained architecture optimized for efficient performance.  |
| **Accuracy ğŸ“Š**              | 97.65% â€“ High accuracy, but may have limitations on more complex data. | 99.52% â€“ Slightly higher accuracy, showing better performance on plant disease detection. |
| **Training Time â³**         | Generally requires more epochs to converge.                     | Faster convergence due to advanced architecture.              |
| **Parameter Count âš™ï¸**      | Fewer parameters, which may reduce computational cost.           | Higher parameter count but more optimized for efficiency.     |
| **Computational Efficiency ğŸ’»** | Less efficient on large datasets, can be slower.                | Highly efficient, especially on large datasets, thanks to the design of EfficientNet. |
| **Overfitting ğŸ”´**           | Can be prone to overfitting with small datasets.                 | Less prone to overfitting, even with a higher number of parameters. |
| **Generalization ğŸ§©**        | May have lower generalization capabilities compared to EfficientNet B3. | Excellent generalization, benefiting from pretraining and its design. |
| **Suitability for Small Datasets ğŸŒ¾** | Adaptable for smaller datasets with proper tuning.              | Suitable for both small and large datasets due to transfer learning. |

#Which is Best? ğŸ¤”

**EfficientNet B3 âš¡**:
With an accuracy of 99.52%, it clearly outperforms the CNN model. It is likely the better choice for plant disease detection due to its superior accuracy, efficient use of parameters, and strong generalization abilities.

**CNNğŸ§ : ** While achieving 97.65% accuracy, it performs slightly lower than EfficientNet B3. It could still be a solid option for simpler or smaller datasets, but for optimal results in detecting plant diseases, EfficientNet B3 would likely offer the best performance.

Conclusion:
With EfficientNet B3 providing a higher accuracy (99.52%), it is the recommended model for your project, especially if the goal is to maximize performance on complex datasets like plant disease detection.

---

### **Short Model Comparison (âœ”ï¸ vs. âŒ)**

| **Metric**                        | **CNN Model**              | **EfficientNetB3 Model**       |
|-----------------------------------|----------------------------|--------------------------------|
| **Accuracy ğŸ†**                    | âŒ Lower (0.9765)           | âœ”ï¸ Higher (0.9952)            |
| **Training Time â³**                | âŒ Longer                  | âœ”ï¸ Faster (due to transfer learning) |
| **Generalization ğŸŒ**              | âŒ May overfit              | âœ”ï¸ Excellent generalization  |
| **Computational Efficiency âš¡**    | âŒ Higher resource use      | âœ”ï¸ More efficient             |
| **Ease of Implementation ğŸ§‘â€ğŸ’»**    | âŒ Requires more customization | âœ”ï¸ Easy (pre-trained model)   |
| **Overfitting Risk âš ï¸**           | âŒ Higher                   | âœ”ï¸ Lower                      |
| **Resource Consumption ğŸ’»**        | âŒ High                     | âœ”ï¸ Low                        |

---

### **Best Choice**: **EfficientNetB3** âœ”ï¸

#**âœ… Conclusion**
In this project, we successfully developed a plant disease detection system using deep learning techniques. Two models were implemented and evaluated: a custom CNN and the EfficientNet B3 architecture.

The CNN model achieved an impressive accuracy of 97.65%, demonstrating its capability in identifying diseases from leaf images.

The EfficientNet B3 model outperformed the CNN with a remarkable accuracy of 99.52%, proving its efficiency, scalability, and superior generalization.

EfficientNet B3â€™s use of compound scaling, Swish activation, and squeeze-and-excitation blocks contributed to its high performance with optimized resource usage.

This project shows that deep learning models, especially EfficientNet B3, are powerful tools for real-time plant disease diagnosis. Such systems can be integrated
into mobile or web applications to assist farmers and agricultural experts, ultimately helping to increase crop yield, reduce losses,
and promote smarter farming practices.
"""